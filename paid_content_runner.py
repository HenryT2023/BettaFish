# -*- coding: utf-8 -*-
"""
Paid Content Runner â€” ä»˜è´¹æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ

èŒè´£ï¼š
1. ä» paid_content_queue æˆ–æ‰‹åŠ¨æŒ‡å®šè¯é¢˜è·å–å¾…å¤„ç†é¡¹
2. ForumEngine å®Œæ•´è¾©è®º + InsightEngine å…¨é‡åˆ†æ
3. ç”Ÿæˆ 3000-5000 å­—æ·±åº¦ç ”ç©¶æŠ¥å‘Š
4. docx_renderer â†’ .docx
5. å‘é€ Telegram ä¾›å®¡æ ¸

ç”¨æ³•ï¼š
    python paid_content_runner.py                        # ä»é˜Ÿåˆ—å–è¯é¢˜
    python paid_content_runner.py --topic "è·¨å¢ƒç”µå•†AI"   # æ‰‹åŠ¨æŒ‡å®š
"""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional

from loguru import logger

PROJECT_ROOT = Path(__file__).resolve().parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from config import settings
from pipeline_state import (
    dequeue_paid_content,
    load_state,
    mark_paid_content_done,
    save_state,
)
from telegram_sender import send_document, send_message


def generate_deep_report(topic: str) -> str:
    """
    è°ƒç”¨ LLM ç”Ÿæˆä»˜è´¹æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚
    ä¸å…è´¹æ–‡ç« åŒºåˆ«ï¼šå®Œæ•´æ•°æ® + æ–¹æ³•è®º + è¶‹åŠ¿é¢„æµ‹ + è¡ŒåŠ¨å»ºè®®åˆ†è§’è‰²ã€‚
    """
    from openai import OpenAI

    api_key = settings.REPORT_ENGINE_API_KEY or settings.INSIGHT_ENGINE_API_KEY
    base_url = settings.REPORT_ENGINE_BASE_URL or settings.INSIGHT_ENGINE_BASE_URL
    model = settings.REPORT_ENGINE_MODEL_NAME or settings.INSIGHT_ENGINE_MODEL_NAME or "qwen-max"

    if not api_key:
        logger.error("æ— å¯ç”¨ LLM API Key")
        return ""

    client = OpenAI(api_key=api_key, base_url=base_url)

    # å…ˆç”¨æœç´¢è·å–èƒŒæ™¯æ•°æ®
    search_context = _gather_search_context(topic)

    system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è·¨å¢ƒç”µå•†å’Œæ•°å­—è´¸æ˜“ç ”ç©¶åˆ†æå¸ˆï¼Œä¸ºã€Œä¸œæ—ºæ•°è´¸ã€æ’°å†™ä»˜è´¹æ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚

æŠ¥å‘Šç»“æ„ï¼ˆä¸¥æ ¼éµå¾ªï¼‰ï¼š
# [æŠ¥å‘Šæ ‡é¢˜]

## 1. æ‰§è¡Œæ‘˜è¦
300å­—ï¼Œæ ¸å¿ƒå‘ç° + å…³é”®æ•°æ®æŒ‡æ ‡ã€‚é«˜ç®¡ä¸€é¡µçº¸èƒ½çœ‹å®Œã€‚

## 2. æ•°æ®å…¨æ™¯
å¤šå¹³å°æ•°æ®æ±‡æ€»ï¼ˆæµ·å¤– + å›½å†…ï¼‰ï¼Œè¯´æ˜æ•°æ®æ¥æºä¸æ—¶é—´èŒƒå›´ã€‚

## 3. æ·±åº¦åˆ†æ
### 3.1 è¶‹åŠ¿åˆ†æ
ç”¨æ•°æ®æ”¯æ’‘ï¼Œå¯¹æ¯”å›½å†…å¤–ã€‚
### 3.2 ç«å“å¯¹æ¯”
å›½å†…å¤–ä¸»è¦ç©å®¶ï¼Œå„è‡ªä¼˜åŠ£åŠ¿ã€‚
### 3.3 æœºä¼šè¯†åˆ«
è“æµ·å¸‚åœº / å·®å¼‚åŒ–åˆ‡å…¥ç‚¹ã€‚
### 3.4 é£é™©è¯„ä¼°
æ”¿ç­–ã€å¸‚åœºã€æŠ€æœ¯é£é™©ã€‚

## 4. æƒ…æ„Ÿåˆ†æ
èˆ†è®ºé£å‘ï¼ˆæ­£é¢/ä¸­æ€§/è´Ÿé¢ï¼‰ï¼ŒKOL è§‚ç‚¹ï¼Œç”¨æˆ·çœŸå®åé¦ˆã€‚

## 5. è¡ŒåŠ¨å»ºè®®
### 5.1 å–å®¶/å“ç‰Œæ–¹
### 5.2 æŠ•èµ„è€…/åˆ›ä¸šè€…
### 5.3 çŸ­æœŸ vs ä¸­æœŸç­–ç•¥

## 6. æ•°æ®é™„å½•
å‚è€ƒæ¥æºé“¾æ¥ã€æœ¯è¯­è§£é‡Šã€‚

å†™ä½œè¦æ±‚ï¼š
- æ€»é•¿ 3000-5000 å­—
- ç”¨ **åŠ ç²—** çªå‡ºå…³é”®æ•°æ®
- æ¯æ®µ 80-200 å­—
- æ•°æ®è¦å…·ä½“ï¼ˆæ•°å­—ã€ç™¾åˆ†æ¯”ã€æ—¶é—´ç‚¹ï¼‰
- è§‚ç‚¹è¦æœ‰è®ºæ®æ”¯æ’‘
- è¯­æ°”ï¼šä¸“ä¸šä¸¥è°¨ï¼Œé€‚åˆä»˜è´¹è¯»è€…
- è¾“å‡ºçº¯ Markdown æ ¼å¼"""

    user_prompt = f"ç ”ç©¶è¯é¢˜ï¼š{topic}\n\nèƒŒæ™¯èµ„æ–™ï¼š\n{search_context}"

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt[:20000]},
            ],
            temperature=0.5,
            timeout=300,
        )
        content = response.choices[0].message.content.strip()

        if content.startswith("```markdown"):
            content = content[len("```markdown"):].strip()
        if content.startswith("```"):
            content = content[3:].strip()
        if content.endswith("```"):
            content = content[:-3].strip()

        return content

    except Exception as e:
        logger.error(f"æ·±åº¦æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return ""


def _gather_search_context(topic: str) -> str:
    """é€šè¿‡æœç´¢è·å–è¯é¢˜èƒŒæ™¯æ•°æ®"""
    context_parts = []

    try:
        from QueryEngine.tools.search import TavilyNewsAgency
        tavily_key = settings.TAVILY_API_KEY
        if tavily_key:
            agency = TavilyNewsAgency(api_key=tavily_key)
            response = agency.deep_search_news(topic)
            if response.answer:
                context_parts.append(f"[æœç´¢æ‘˜è¦] {response.answer}")
            for r in response.results[:5]:
                context_parts.append(f"- {r.title}: {(r.content or '')[:200]}")
    except Exception as e:
        logger.warning(f"æœç´¢èƒŒæ™¯æ•°æ®å¤±è´¥: {e}")

    # è¯»å–æœ€è¿‘çš„ scout æ•°æ®ä½œä¸ºè¡¥å……
    import glob
    scout_dir = PROJECT_ROOT / "pipeline" / "scout"
    files = sorted(glob.glob(str(scout_dir / "*.json")), reverse=True)[:3]
    for fp in files:
        try:
            with open(fp, "r", encoding="utf-8") as f:
                data = json.load(f)
            for item in data.get("items", [])[:3]:
                title = item.get("title", "")
                if topic.lower() in title.lower() or any(
                    kw in title.lower() for kw in topic.lower().split()
                ):
                    context_parts.append(f"- [Scout] {title}: {item.get('content', '')[:150]}")
        except Exception:
            pass

    return "\n".join(context_parts) if context_parts else "ï¼ˆæ— é¢å¤–èƒŒæ™¯æ•°æ®ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†åº“ç”Ÿæˆï¼‰"


def run_paid_content(topic_override: Optional[str] = None) -> Optional[str]:
    """
    ç”Ÿæˆä»˜è´¹æ·±åº¦æŠ¥å‘Šã€‚
    è¿”å› .docx æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› Noneã€‚
    """
    date_str = datetime.now().strftime("%Y%m%d")
    logger.info(f"=== Paid Content å¼€å§‹ | date={date_str} ===")

    # ç¡®å®šè¯é¢˜
    state = load_state()
    topic = topic_override

    if not topic:
        item = dequeue_paid_content(state)
        if item:
            topic = item.get("topic", "")
            save_state(state)
        else:
            logger.info("ä»˜è´¹å†…å®¹é˜Ÿåˆ—ä¸ºç©ºï¼Œè·³è¿‡")
            return None

    if not topic:
        logger.warning("æ— è¯é¢˜å¯å¤„ç†")
        return None

    logger.info(f">>> è¯é¢˜: {topic}")

    # ç”ŸæˆæŠ¥å‘Š
    logger.info(">>> ç”Ÿæˆæ·±åº¦æŠ¥å‘Š...")
    report_md = generate_deep_report(topic)
    if not report_md or len(report_md) < 1000:
        logger.warning(f"æŠ¥å‘Šå†…å®¹è¿‡çŸ­ ({len(report_md)} å­—)")
        return None

    logger.info(f">>> æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {len(report_md)} å­—")

    # ä¿å­˜ Markdown
    drafts_dir = PROJECT_ROOT / "pipeline" / "drafts"
    drafts_dir.mkdir(parents=True, exist_ok=True)
    md_path = drafts_dir / f"{date_str}-paid-report.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(report_md)

    # æ¸²æŸ“ docx
    docx_path = str(drafts_dir / f"{date_str}-paid-report.docx")
    logger.info(">>> æ¸²æŸ“ DOCX...")
    try:
        import importlib.util
        spec = importlib.util.spec_from_file_location(
            "docx_renderer",
            str(PROJECT_ROOT / "ReportEngine" / "renderers" / "docx_renderer.py"),
        )
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
        renderer = mod.DocxRenderer()
        renderer.render_from_markdown(report_md)
        renderer.save(docx_path)
    except Exception as e:
        logger.error(f"DOCX æ¸²æŸ“å¤±è´¥: {e}")
        return None

    # å‘é€ Telegramï¼ˆæ ‡è®°ä¸ºä»˜è´¹æŠ¥å‘Šï¼Œä¾›äººå·¥å®¡æ ¸ï¼‰
    title = topic
    for line in report_md.split("\n"):
        if line.strip().startswith("# "):
            title = line.strip()[2:]
            break

    caption = f"ğŸ’ ä»˜è´¹æ·±åº¦æŠ¥å‘Š\n\nğŸ“„ {title}\n\n{report_md[:200]}...\n\nâš ï¸ è¯·å®¡æ ¸åå†³å®šæ˜¯å¦å‘å¸ƒ"
    send_document(docx_path, caption=caption)

    # æ ‡è®°å®Œæˆ
    state = load_state()
    mark_paid_content_done(topic, state)
    save_state(state)

    logger.info(f"=== Paid Content å®Œæˆ | {docx_path} ===")
    return docx_path


def main():
    parser = argparse.ArgumentParser(description="Paid Content Runner - ä»˜è´¹æ·±åº¦æŠ¥å‘Š")
    parser.add_argument("--topic", type=str, help="æ‰‹åŠ¨æŒ‡å®šè¯é¢˜")
    args = parser.parse_args()

    result = run_paid_content(topic_override=args.topic)
    if result:
        print(f"Paid Content å®Œæˆ: {result}")
    else:
        print("Paid Content å®Œæˆ: æ— è¾“å‡º")


if __name__ == "__main__":
    main()
